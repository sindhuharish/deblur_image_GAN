{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "##### LOss Function ########\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "# Note the image_shape must be multiple of patch_shape\n",
    "image_shape = (256, 256, 3)\n",
    "\n",
    "\n",
    "def l1_loss(y_true, y_pred):\n",
    "    return K.mean(K.abs(y_pred - y_true))\n",
    "\n",
    "\n",
    "def perceptual_loss_100(y_true, y_pred):\n",
    "    return 100 * perceptual_loss(y_true, y_pred)\n",
    "\n",
    "\n",
    "def perceptual_loss(y_true, y_pred):\n",
    "    vgg = VGG16(include_top=False, weights='imagenet', input_shape=image_shape)\n",
    "    loss_model = Model(inputs=vgg.input, outputs=vgg.get_layer('block3_conv3').output)\n",
    "    loss_model.trainable = False\n",
    "    return K.mean(K.square(loss_model(y_true) - loss_model(y_pred)))\n",
    "\n",
    "\n",
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return K.mean(y_true*y_pred)\n",
    "\n",
    "\n",
    "def gradient_penalty_loss(self, y_true, y_pred, averaged_samples):\n",
    "    gradients = K.gradients(y_pred, averaged_samples)[0]\n",
    "    gradients_sqr = K.square(gradients)\n",
    "    gradients_sqr_sum = K.sum(gradients_sqr,\n",
    "                              axis=np.arange(1, len(gradients_sqr.shape)))\n",
    "\n",
    "    gradient_l2_norm = K.sqrt(gradients_sqr_sum)\n",
    "    gradient_penalty = K.square(1 - gradient_l2_norm)\n",
    "\n",
    "    return K.mean(gradient_penalty)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########  Res Block ###############\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.engine import InputSpec\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers import Input, Conv2D, Activation, BatchNormalization\n",
    "from keras.layers.merge import Add\n",
    "from keras.utils import conv_utils\n",
    "from keras.layers.core import Dropout\n",
    "\n",
    "\n",
    "def res_block(input, filters, kernel_size=(3, 3), strides=(1, 1), use_dropout=False):\n",
    "    \"\"\"\n",
    "    Instanciate a Keras Resnet Block using sequential API.\n",
    "\n",
    "    :param input: Input tensor\n",
    "    :param filters: Number of filters to use\n",
    "    :param kernel_size: Shape of the kernel for the convolution\n",
    "    :param strides: Shape of the strides for the convolution\n",
    "    :param use_dropout: Boolean value to determine the use of dropout\n",
    "    :return: Keras Model\n",
    "    \"\"\"\n",
    "    x = ReflectionPadding2D((1, 1))(input)\n",
    "    x = Conv2D(filters=filters,\n",
    "               kernel_size=kernel_size,\n",
    "               strides=strides,)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    if use_dropout:\n",
    "        x = Dropout(0.5)(x)\n",
    "\n",
    "    x = ReflectionPadding2D((1, 1))(x)\n",
    "    x = Conv2D(filters=filters,\n",
    "               kernel_size=kernel_size,\n",
    "               strides=strides,)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    merged = Add()([input, x])\n",
    "    return merged\n",
    "\n",
    "\n",
    "def spatial_reflection_2d_padding(x, padding=((1, 1), (1, 1)), data_format=None):\n",
    "    \"\"\"\n",
    "    Pad the 2nd and 3rd dimensions of a 4D tensor.\n",
    "\n",
    "    :param x: Input tensor\n",
    "    :param padding: Shape of padding to use\n",
    "    :param data_format: Tensorflow vs Theano convention ('channels_last', 'channels_first')\n",
    "    :return: Tensorflow tensor\n",
    "    \"\"\"\n",
    "    assert len(padding) == 2\n",
    "    assert len(padding[0]) == 2\n",
    "    assert len(padding[1]) == 2\n",
    "    if data_format is None:\n",
    "        data_format = image_data_format()\n",
    "    if data_format not in {'channels_first', 'channels_last'}:\n",
    "        raise ValueError('Unknown data_format ' + str(data_format))\n",
    "\n",
    "    if data_format == 'channels_first':\n",
    "        pattern = [[0, 0],\n",
    "                   [0, 0],\n",
    "                   list(padding[0]),\n",
    "                   list(padding[1])]\n",
    "    else:\n",
    "        pattern = [[0, 0],\n",
    "                   list(padding[0]), list(padding[1]),\n",
    "                   [0, 0]]\n",
    "    return tf.pad(x, pattern, \"REFLECT\")\n",
    "\n",
    "\n",
    "# TODO: Credits\n",
    "class ReflectionPadding2D(Layer):\n",
    "    \"\"\"Reflection-padding layer for 2D input\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 padding=(1, 1),\n",
    "                 data_format=None,\n",
    "                 **kwargs):\n",
    "        super(ReflectionPadding2D, self).__init__(**kwargs)\n",
    "        self.data_format = conv_utils.normalize_data_format(data_format)\n",
    "        if isinstance(padding, int):\n",
    "            self.padding = ((padding, padding), (padding, padding))\n",
    "        elif hasattr(padding, '__len__'):\n",
    "            if len(padding) != 2:\n",
    "                raise ValueError('`padding` should have two elements. '\n",
    "                                 'Found: ' + str(padding))\n",
    "            height_padding = conv_utils.normalize_tuple(padding[0], 2,\n",
    "                                                        '1st entry of padding')\n",
    "            width_padding = conv_utils.normalize_tuple(padding[1], 2,\n",
    "                                                       '2nd entry of padding')\n",
    "            self.padding = (height_padding, width_padding)\n",
    "        else:\n",
    "            raise ValueError('`padding` should be either an int, '\n",
    "                             'a tuple of 2 ints '\n",
    "                             '(symmetric_height_pad, symmetric_width_pad), '\n",
    "                             'or a tuple of 2 tuples of 2 ints '\n",
    "                             '((top_pad, bottom_pad), (left_pad, right_pad)). '\n",
    "                             'Found: ' + str(padding))\n",
    "        self.input_spec = InputSpec(ndim=4)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.data_format == 'channels_first':\n",
    "            if input_shape[2] is not None:\n",
    "                rows = input_shape[2] + self.padding[0][0] + self.padding[0][1]\n",
    "            else:\n",
    "                rows = None\n",
    "            if input_shape[3] is not None:\n",
    "                cols = input_shape[3] + self.padding[1][0] + self.padding[1][1]\n",
    "            else:\n",
    "                cols = None\n",
    "            return (input_shape[0],\n",
    "                    input_shape[1],\n",
    "                    rows,\n",
    "                    cols)\n",
    "        elif self.data_format == 'channels_last':\n",
    "            if input_shape[1] is not None:\n",
    "                rows = input_shape[1] + self.padding[0][0] + self.padding[0][1]\n",
    "            else:\n",
    "                rows = None\n",
    "            if input_shape[2] is not None:\n",
    "                cols = input_shape[2] + self.padding[1][0] + self.padding[1][1]\n",
    "            else:\n",
    "                cols = None\n",
    "            return (input_shape[0],\n",
    "                    rows,\n",
    "                    cols,\n",
    "                    input_shape[3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return spatial_reflection_2d_padding(inputs,\n",
    "                                             padding=self.padding,\n",
    "                                             data_format=self.data_format)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'padding': self.padding,\n",
    "                  'data_format': self.data_format}\n",
    "        base_config = super(ReflectionPadding2D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Model ############\n",
    "\n",
    "\n",
    "from keras.layers import Input, Activation, Add, UpSampling2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.core import Dense, Flatten, Lambda\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "\n",
    "# the paper defined hyper-parameter:chr\n",
    "channel_rate = 64\n",
    "# Note the image_shape must be multiple of patch_shape\n",
    "image_shape = (256, 256, 3)\n",
    "patch_shape = (channel_rate, channel_rate, 3)\n",
    "\n",
    "ngf = 64\n",
    "ndf = 64\n",
    "input_nc = 3\n",
    "output_nc = 3\n",
    "input_shape_generator = (256, 256, input_nc)\n",
    "input_shape_discriminator = (256, 256, output_nc)\n",
    "n_blocks_gen = 9\n",
    "\n",
    "\n",
    "def generator_model():\n",
    "    \"\"\"Build generator architecture.\"\"\"\n",
    "    # Current version : ResNet block\n",
    "    inputs = Input(shape=image_shape)\n",
    "\n",
    "    x = ReflectionPadding2D((3, 3))(inputs)\n",
    "    x = Conv2D(filters=ngf, kernel_size=(7, 7), padding='valid')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    n_downsampling = 2\n",
    "    for i in range(n_downsampling):\n",
    "        mult = 2**i\n",
    "        x = Conv2D(filters=ngf*mult*2, kernel_size=(3, 3), strides=2, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "\n",
    "    mult = 2**n_downsampling\n",
    "    for i in range(n_blocks_gen):\n",
    "        x = res_block(x, ngf*mult, use_dropout=True)\n",
    "\n",
    "    for i in range(n_downsampling):\n",
    "        mult = 2**(n_downsampling - i)\n",
    "        # x = Conv2DTranspose(filters=int(ngf * mult / 2), kernel_size=(3, 3), strides=2, padding='same')(x)\n",
    "        x = UpSampling2D()(x)\n",
    "        x = Conv2D(filters=int(ngf * mult / 2), kernel_size=(3, 3), padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "\n",
    "    x = ReflectionPadding2D((3, 3))(x)\n",
    "    x = Conv2D(filters=output_nc, kernel_size=(7, 7), padding='valid')(x)\n",
    "    x = Activation('tanh')(x)\n",
    "\n",
    "    outputs = Add()([x, inputs])\n",
    "    # outputs = Lambda(lambda z: K.clip(z, -1, 1))(x)\n",
    "    outputs = Lambda(lambda z: z/2)(outputs)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs, name='Generator')\n",
    "    return model\n",
    "\n",
    "\n",
    "def discriminator_model():\n",
    "    \"\"\"Build discriminator architecture.\"\"\"\n",
    "    n_layers, use_sigmoid = 3, False\n",
    "    inputs = Input(shape=input_shape_discriminator)\n",
    "\n",
    "    x = Conv2D(filters=ndf, kernel_size=(4, 4), strides=2, padding='same')(inputs)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "\n",
    "    nf_mult, nf_mult_prev = 1, 1\n",
    "    for n in range(n_layers):\n",
    "        nf_mult_prev, nf_mult = nf_mult, min(2**n, 8)\n",
    "        x = Conv2D(filters=ndf*nf_mult, kernel_size=(4, 4), strides=2, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(0.2)(x)\n",
    "\n",
    "    nf_mult_prev, nf_mult = nf_mult, min(2**n_layers, 8)\n",
    "    x = Conv2D(filters=ndf*nf_mult, kernel_size=(4, 4), strides=1, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "\n",
    "    x = Conv2D(filters=1, kernel_size=(4, 4), strides=1, padding='same')(x)\n",
    "    if use_sigmoid:\n",
    "        x = Activation('sigmoid')(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1024, activation='tanh')(x)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=x, name='Discriminator')\n",
    "    return model\n",
    "\n",
    "\n",
    "def generator_containing_discriminator(generator, discriminator):\n",
    "    inputs = Input(shape=image_shape)\n",
    "    generated_image = generator(inputs)\n",
    "    outputs = discriminator(generated_image)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def generator_containing_discriminator_multiple_outputs(generator, discriminator):\n",
    "    inputs = Input(shape=image_shape)\n",
    "    generated_image = generator(inputs)\n",
    "    outputs = discriminator(generated_image)\n",
    "    model = Model(inputs=inputs, outputs=[generated_image, outputs])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:68: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:504: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3828: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:166: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:171: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1794: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3135: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1937: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
      "\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 256, 256, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reflection_padding2d_1 (Reflect (None, 262, 262, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 256, 256, 64) 9472        reflection_padding2d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 256, 256, 64) 256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 256, 256, 64) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 128, 128, 128 73856       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 128, 128, 128 512         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 128, 128, 128 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 64, 64, 256)  295168      activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, 64, 256)  1024        conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 64, 64, 256)  0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "reflection_padding2d_2 (Reflect (None, 66, 66, 256)  0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 64, 64, 256)  590080      reflection_padding2d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 64, 64, 256)  1024        conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 64, 64, 256)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 64, 64, 256)  0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reflection_padding2d_3 (Reflect (None, 66, 66, 256)  0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 64, 64, 256)  590080      reflection_padding2d_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 64, 64, 256)  1024        conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 64, 64, 256)  0           activation_3[0][0]               \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "reflection_padding2d_4 (Reflect (None, 66, 66, 256)  0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 64, 64, 256)  590080      reflection_padding2d_4[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 64, 64, 256)  1024        conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 64, 64, 256)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 64, 64, 256)  0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reflection_padding2d_5 (Reflect (None, 66, 66, 256)  0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 64, 64, 256)  590080      reflection_padding2d_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 64, 64, 256)  1024        conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 64, 64, 256)  0           add_1[0][0]                      \n",
      "                                                                 batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "reflection_padding2d_6 (Reflect (None, 66, 66, 256)  0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 64, 64, 256)  590080      reflection_padding2d_6[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 64, 64, 256)  1024        conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 64, 64, 256)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 64, 64, 256)  0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reflection_padding2d_7 (Reflect (None, 66, 66, 256)  0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 64, 64, 256)  590080      reflection_padding2d_7[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 64, 64, 256)  1024        conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 64, 64, 256)  0           add_2[0][0]                      \n",
      "                                                                 batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "reflection_padding2d_8 (Reflect (None, 66, 66, 256)  0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 64, 64, 256)  590080      reflection_padding2d_8[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 64, 64, 256)  1024        conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 64, 64, 256)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 64, 64, 256)  0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reflection_padding2d_9 (Reflect (None, 66, 66, 256)  0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 64, 64, 256)  590080      reflection_padding2d_9[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 64, 64, 256)  1024        conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 64, 64, 256)  0           add_3[0][0]                      \n",
      "                                                                 batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "reflection_padding2d_10 (Reflec (None, 66, 66, 256)  0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 64, 64, 256)  590080      reflection_padding2d_10[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 64, 64, 256)  1024        conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 64, 64, 256)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 64, 64, 256)  0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reflection_padding2d_11 (Reflec (None, 66, 66, 256)  0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 64, 64, 256)  590080      reflection_padding2d_11[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 64, 64, 256)  1024        conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 64, 64, 256)  0           add_4[0][0]                      \n",
      "                                                                 batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "reflection_padding2d_12 (Reflec (None, 66, 66, 256)  0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 64, 64, 256)  590080      reflection_padding2d_12[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 64, 64, 256)  1024        conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 64, 64, 256)  0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 64, 64, 256)  0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reflection_padding2d_13 (Reflec (None, 66, 66, 256)  0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 64, 64, 256)  590080      reflection_padding2d_13[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 64, 64, 256)  1024        conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 64, 64, 256)  0           add_5[0][0]                      \n",
      "                                                                 batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "reflection_padding2d_14 (Reflec (None, 66, 66, 256)  0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 64, 64, 256)  590080      reflection_padding2d_14[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 64, 64, 256)  1024        conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 64, 64, 256)  0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 64, 64, 256)  0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reflection_padding2d_15 (Reflec (None, 66, 66, 256)  0           dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 64, 64, 256)  590080      reflection_padding2d_15[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 64, 64, 256)  1024        conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 64, 64, 256)  0           add_6[0][0]                      \n",
      "                                                                 batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "reflection_padding2d_16 (Reflec (None, 66, 66, 256)  0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 64, 64, 256)  590080      reflection_padding2d_16[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 64, 64, 256)  1024        conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 64, 64, 256)  0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 64, 64, 256)  0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reflection_padding2d_17 (Reflec (None, 66, 66, 256)  0           dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 64, 64, 256)  590080      reflection_padding2d_17[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 64, 64, 256)  1024        conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 64, 64, 256)  0           add_7[0][0]                      \n",
      "                                                                 batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "reflection_padding2d_18 (Reflec (None, 66, 66, 256)  0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 64, 64, 256)  590080      reflection_padding2d_18[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 64, 64, 256)  1024        conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 64, 64, 256)  0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 64, 64, 256)  0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reflection_padding2d_19 (Reflec (None, 66, 66, 256)  0           dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 64, 64, 256)  590080      reflection_padding2d_19[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 64, 64, 256)  1024        conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 64, 64, 256)  0           add_8[0][0]                      \n",
      "                                                                 batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 128, 128, 256 0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 128, 128, 128 295040      up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 128, 128, 128 512         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 128, 128, 128 0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 256, 256, 128 0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 256, 256, 64) 73792       up_sampling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 256, 256, 64) 256         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 256, 256, 64) 0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "reflection_padding2d_20 (Reflec (None, 262, 262, 64) 0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 256, 256, 3)  9411        reflection_padding2d_20[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 256, 256, 3)  0           conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 256, 256, 3)  0           activation_15[0][0]              \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 256, 256, 3)  0           add_10[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 11,399,171\n",
      "Trainable params: 11,388,675\n",
      "Non-trainable params: 10,496\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 256, 256, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 128, 128, 64)      3136      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 64, 64, 64)        65600     \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 64, 64, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 32, 32, 128)       131200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 16, 16, 256)       524544    \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 16, 16, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 16, 16, 512)       2097664   \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 16, 16, 512)       2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 16, 16, 1)         8193      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              263168    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 1025      \n",
      "=================================================================\n",
      "Total params: 3,098,370\n",
      "Trainable params: 3,096,450\n",
      "Non-trainable params: 1,920\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 256, 256, 3)       0         \n",
      "_________________________________________________________________\n",
      "Generator (Model)            (None, 256, 256, 3)       11399171  \n",
      "_________________________________________________________________\n",
      "Discriminator (Model)        (None, 1)                 3098370   \n",
      "=================================================================\n",
      "Total params: 14,497,541\n",
      "Trainable params: 14,485,125\n",
      "Non-trainable params: 12,416\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "g = generator_model()\n",
    "g.summary()\n",
    "d = discriminator_model()\n",
    "d.summary()\n",
    "m = generator_containing_discriminator(generator_model(), discriminator_model())\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Train Network ################\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import click\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from utils import load_images, write_log\n",
    "\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "BASE_DIR = 'weights/'\n",
    "\n",
    "\n",
    "def save_all_weights(d, g, epoch_number, current_loss):\n",
    "    now = datetime.datetime.now()\n",
    "    save_dir = os.path.join(BASE_DIR, '{}{}'.format(now.month, now.day))\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    g.save_weights(os.path.join(save_dir, 'generator_{}_{}.h5'.format(epoch_number, current_loss)), True)\n",
    "    d.save_weights(os.path.join(save_dir, 'discriminator_{}.h5'.format(epoch_number)), True)\n",
    "\n",
    "\n",
    "def train_multiple_outputs(n_images, batch_size, log_dir, epoch_num, critic_updates=5):\n",
    "    data = load_images('images/train', n_images)\n",
    "    y_train, x_train = data['B'], data['A']\n",
    "\n",
    "    g = generator_model()\n",
    "    d = discriminator_model()\n",
    "    d_on_g = generator_containing_discriminator_multiple_outputs(g, d)\n",
    "\n",
    "    d_opt = Adam(lr=1E-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    d_on_g_opt = Adam(lr=1E-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "    d.trainable = True\n",
    "    d.compile(optimizer=d_opt, loss=wasserstein_loss)\n",
    "    d.trainable = False\n",
    "    loss = [perceptual_loss, wasserstein_loss]\n",
    "    loss_weights = [100, 1]\n",
    "    d_on_g.compile(optimizer=d_on_g_opt, loss=loss, loss_weights=loss_weights)\n",
    "    d.trainable = True\n",
    "\n",
    "    output_true_batch, output_false_batch = np.ones((batch_size, 1)), -np.ones((batch_size, 1))\n",
    "\n",
    "    log_path = 'logs'\n",
    "    tensorboard_callback = TensorBoard(log_path)\n",
    "\n",
    "    for epoch in tqdm.tqdm(range(epoch_num)):\n",
    "        permutated_indexes = np.random.permutation(x_train.shape[0])\n",
    "\n",
    "        d_losses = []\n",
    "        d_on_g_losses = []\n",
    "        for index in range(int(x_train.shape[0] / batch_size)):\n",
    "            batch_indexes = permutated_indexes[index*batch_size:(index+1)*batch_size]\n",
    "            image_blur_batch = x_train[batch_indexes]\n",
    "            image_full_batch = y_train[batch_indexes]\n",
    "\n",
    "            generated_images = g.predict(x=image_blur_batch, batch_size=batch_size)\n",
    "\n",
    "            for _ in range(critic_updates):\n",
    "                d_loss_real = d.train_on_batch(image_full_batch, output_true_batch)\n",
    "                d_loss_fake = d.train_on_batch(generated_images, output_false_batch)\n",
    "                d_loss = 0.5 * np.add(d_loss_fake, d_loss_real)\n",
    "                d_losses.append(d_loss)\n",
    "\n",
    "            d.trainable = False\n",
    "\n",
    "            d_on_g_loss = d_on_g.train_on_batch(image_blur_batch, [image_full_batch, output_true_batch])\n",
    "            d_on_g_losses.append(d_on_g_loss)\n",
    "\n",
    "            d.trainable = True\n",
    "\n",
    "        # write_log(tensorboard_callback, ['g_loss', 'd_on_g_loss'], [np.mean(d_losses), np.mean(d_on_g_losses)], epoch_num)\n",
    "        print(np.mean(d_losses), np.mean(d_on_g_losses))\n",
    "        with open('log.txt', 'a+') as f:\n",
    "            f.write('{} - {} - {}\\n'.format(epoch, np.mean(d_losses), np.mean(d_on_g_losses)))\n",
    "\n",
    "        save_all_weights(d, g, epoch, int(np.mean(d_on_g_losses)))\n",
    "\n",
    "def train(n_images, batch_size, log_dir, epoch_num, critic_updates):\n",
    "    return train_multiple_outputs(n_images, batch_size, log_dir, epoch_num, critic_updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Initilize Training Parameters\n",
    "\n",
    "n_images = 512\n",
    "batch_size = 4\n",
    "log_dir = 'log'\n",
    "epoch_num = 70\n",
    "critic_updates = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(n_images, batch_size, log_dir, epoch_num, critic_updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import click\n",
    "import os\n",
    "from utils import preprocess_image, deprocess_image, load_image\n",
    "\n",
    "def deblur(weight_path, input_dir, output_dir):\n",
    "    g = generator_model()\n",
    "    g.load_weights(weight_path)\n",
    "    for image_name in os.listdir(input_dir):\n",
    "        image = np.array([preprocess_image(load_image(os.path.join(input_dir, image_name)))])\n",
    "        x_test = image\n",
    "        generated_images = g.predict(x=x_test)\n",
    "        generated = np.array([deprocess_image(img) for img in generated_images])\n",
    "        x_test = deprocess_image(x_test)\n",
    "        for i in range(generated_images.shape[0]):\n",
    "            x = x_test[i, :, :, :]\n",
    "            img = generated[i, :, :, :]\n",
    "            output = np.concatenate((x, img), axis=1)\n",
    "            im = Image.fromarray(output.astype(np.uint8))\n",
    "            im.save(os.path.join(output_dir, image_name))\n",
    "\n",
    "\n",
    "weight_path = 'weights/126/generator_59_498.h5'\n",
    "input_dir = 'manual_test'\n",
    "output_dir = 'manual_output'\n",
    "\n",
    "deblur(weight_path, input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
